{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0898053-8e70-44ea-94b6-2506e53ad9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.utils.data as data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from nets import *\n",
    "import time, os, copy, argparse\n",
    "import multiprocessing\n",
    "from torchsummary import summary\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d227fa4e-a616-477c-9b2d-68ae39e9a2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuning the model...\n"
     ]
    }
   ],
   "source": [
    "# construct argument parser \n",
    "# ===============================================================\n",
    "# # Directly set the training mode\n",
    "train_mode = 'finetune'  # Can be 'finetune', 'transfer', or 'scratch'\n",
    "\n",
    "# Assume following uses of train_mode in your script\n",
    "if train_mode == 'finetune':\n",
    "    # Load a pretrained model and modify it for finetuning\n",
    "    print(\"Finetuning the model...\")\n",
    "elif train_mode == 'transfer':\n",
    "    # Setup for transfer learning\n",
    "    print(\"Setting up for transfer learning...\")\n",
    "elif train_mode == 'scratch':\n",
    "    # Build and train model from scratch\n",
    "    print(\"Training model from scratch...\")\n",
    "#===============================================================\n",
    "# ap = argparse.ArgumentParser()\n",
    "# ap.add_argument(\"--mode\", required=True, help=\"Training mode: finetune/transfer/scratch\")\n",
    "# args= vars(ap.parse_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3c35939-4830-4c59-a3ec-bbf96151287d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['0657', '0660', '0661', '0663', '0665', '0666', '0667', '0668', '0669', '0672', '0673', '0674', '0675', '0679', '0680', '0681', '0684', '0687', '0689', '0690', '0692', '0693', '0697', '0699', '0700', '0701', '0703', '0705', '0706', '0707', '0712', '0713', '0714', '0715', '0716', '0722', '0725', '0726', '0728', '0731', '0732', '0734', '0735', '0740', '0741', '0742', '0743', '0747', '0748', '0749', '0750', '0751', '0752', '0753', '0754', '0755', '0757', '0758', '0759', '0760', '0764', '0765', '0766', '0768', '0771', '0772', '0773', '0774', '0775', '0776', '0777', '0778', '0780', '0781', '0782', '0786', '0793', '0796', '0797', '0798', '0802', '0803', '0807', '0808', '0816', '0817', '0818', '0820', '0821', '0822', '0824', '0827', '0828', '0832', '0834', '0841', '0844', '0847', '0848', '0853', '0854', '0858', '0860', '0861', '0864', '0866', '0867', '0868', '0869', '0870', '0873', '0874', '0875', '0878', '0885', '0886', '0887', '0891', '0892', '0894', '0897', '0898', '0899', '0902', '0904', '0905', '0907', '0909', '0910', '0912', '0915', '0920', '0923', '0924', '0925', '0926', '0929', '0930', '0931', '0934', '0935', '0938', '0941', '0943', '0948', '0949', '0950', '0952', '0954', '0955', '0956', '0958', '0962', '0963', '0967', '0968', '0969', '0972', '0976', '0977', '0979', '0984', '0985', '0986', '0987', '0989', '0991', '0993', '0999', '1001', '1005', '1006', '1009', '1011', '1016', '1017', '1019', '1022', '1024', '1025', '1026', '1027', '1028', '1029', '1031', '1033', '1035', '1037', '1038', '1039', '1043', '1047', '1049', '1050', '1057', '1060', '1062', '1063', '1065', '1069', '1070', '1071', '1076', '1079', '1086', '1090', '1091', '1092', '1093', '1096', '1099', '1100', '1101', '1105', '1106', '1107', '1109', '1110', '1112', '1113', '1115', '1123', '1125', '1127', '1128', '1129', '1130', '1137', '1138', '1141', '1142', '1143', '1147', '1148', '1149', '1153', '1154', '1158', '1159', '1161', '1164', '1166', '1171', '1172', '1173', '1174', '1182', '1183', '1184', '1185', '1190', '1191', '1195', '1197', '1198', '1199', '1200', '1201', '1202', '1203', '1214', '1216', '1222', '1228', '1229', '1233', '1235', '1236', '1241', '1243', '1245', '1246', '1247', '1255', '1256', '1258', '1259', '1261', '1262', '1263', '1265', '1267', '1268', '1271', '1272', '1276', '1281', '1282', '1283', '1284', '1286', '1289', '1291', '1292', '1293', '1294', '1297', '1306', '1308', '1309']\n",
      "Training-set size: 8923 \n",
      "Validation-set size: 1498\n",
      "\n",
      "Loading resnet18 for finetuning ...\n",
      "\n",
      "Model Summary:-\n",
      "\n",
      "0 conv1.weight True\n",
      "1 bn1.weight True\n",
      "2 bn1.bias True\n",
      "3 layer1.0.conv1.weight True\n",
      "4 layer1.0.bn1.weight True\n",
      "5 layer1.0.bn1.bias True\n",
      "6 layer1.0.conv2.weight True\n",
      "7 layer1.0.bn2.weight True\n",
      "8 layer1.0.bn2.bias True\n",
      "9 layer1.1.conv1.weight True\n",
      "10 layer1.1.bn1.weight True\n",
      "11 layer1.1.bn1.bias True\n",
      "12 layer1.1.conv2.weight True\n",
      "13 layer1.1.bn2.weight True\n",
      "14 layer1.1.bn2.bias True\n",
      "15 layer2.0.conv1.weight True\n",
      "16 layer2.0.bn1.weight True\n",
      "17 layer2.0.bn1.bias True\n",
      "18 layer2.0.conv2.weight True\n",
      "19 layer2.0.bn2.weight True\n",
      "20 layer2.0.bn2.bias True\n",
      "21 layer2.0.downsample.0.weight True\n",
      "22 layer2.0.downsample.1.weight True\n",
      "23 layer2.0.downsample.1.bias True\n",
      "24 layer2.1.conv1.weight True\n",
      "25 layer2.1.bn1.weight True\n",
      "26 layer2.1.bn1.bias True\n",
      "27 layer2.1.conv2.weight True\n",
      "28 layer2.1.bn2.weight True\n",
      "29 layer2.1.bn2.bias True\n",
      "30 layer3.0.conv1.weight True\n",
      "31 layer3.0.bn1.weight True\n",
      "32 layer3.0.bn1.bias True\n",
      "33 layer3.0.conv2.weight True\n",
      "34 layer3.0.bn2.weight True\n",
      "35 layer3.0.bn2.bias True\n",
      "36 layer3.0.downsample.0.weight True\n",
      "37 layer3.0.downsample.1.weight True\n",
      "38 layer3.0.downsample.1.bias True\n",
      "39 layer3.1.conv1.weight True\n",
      "40 layer3.1.bn1.weight True\n",
      "41 layer3.1.bn1.bias True\n",
      "42 layer3.1.conv2.weight True\n",
      "43 layer3.1.bn2.weight True\n",
      "44 layer3.1.bn2.bias True\n",
      "45 layer4.0.conv1.weight True\n",
      "46 layer4.0.bn1.weight True\n",
      "47 layer4.0.bn1.bias True\n",
      "48 layer4.0.conv2.weight True\n",
      "49 layer4.0.bn2.weight True\n",
      "50 layer4.0.bn2.bias True\n",
      "51 layer4.0.downsample.0.weight True\n",
      "52 layer4.0.downsample.1.weight True\n",
      "53 layer4.0.downsample.1.bias True\n",
      "54 layer4.1.conv1.weight True\n",
      "55 layer4.1.bn1.weight True\n",
      "56 layer4.1.bn1.bias True\n",
      "57 layer4.1.conv2.weight True\n",
      "58 layer4.1.bn2.weight True\n",
      "59 layer4.1.bn2.bias True\n",
      "60 fc.weight True\n",
      "61 fc.bias True\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
      "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
      "             ReLU-14           [-1, 64, 56, 56]               0\n",
      "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
      "             ReLU-17           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
      "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
      "             ReLU-21          [-1, 128, 28, 28]               0\n",
      "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
      "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
      "             ReLU-26          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
      "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
      "             ReLU-30          [-1, 128, 28, 28]               0\n",
      "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
      "             ReLU-33          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
      "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
      "             ReLU-37          [-1, 256, 14, 14]               0\n",
      "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
      "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
      "             ReLU-42          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
      "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
      "             ReLU-46          [-1, 256, 14, 14]               0\n",
      "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
      "             ReLU-49          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
      "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-53            [-1, 512, 7, 7]               0\n",
      "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-58            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
      "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-62            [-1, 512, 7, 7]               0\n",
      "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-65            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 300]         153,900\n",
      "================================================================\n",
      "Total params: 11,330,412\n",
      "Trainable params: 11,330,412\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 62.79\n",
      "Params size (MB): 43.22\n",
      "Estimated Total Size (MB): 106.58\n",
      "----------------------------------------------------------------\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=300, bias=True)\n",
      ")\n",
      "\n",
      "Training:-\n",
      "\n",
      "Epoch 0/14\n",
      "----------\n",
      "train Loss: 5.7500 Acc: 0.0091\n",
      "valid Loss: 5.4723 Acc: 0.0120\n",
      "\n",
      "Epoch 1/14\n",
      "----------\n",
      "train Loss: 5.2699 Acc: 0.0244\n",
      "valid Loss: 5.2051 Acc: 0.0287\n",
      "\n",
      "Epoch 2/14\n",
      "----------\n",
      "train Loss: 4.7487 Acc: 0.0646\n",
      "valid Loss: 4.6805 Acc: 0.0614\n",
      "\n",
      "Epoch 3/14\n",
      "----------\n",
      "train Loss: 3.9857 Acc: 0.1590\n",
      "valid Loss: 4.4215 Acc: 0.1041\n",
      "\n",
      "Epoch 4/14\n",
      "----------\n",
      "train Loss: 3.1786 Acc: 0.2894\n",
      "valid Loss: 3.8802 Acc: 0.1622\n",
      "\n",
      "Epoch 5/14\n",
      "----------\n",
      "train Loss: 2.4775 Acc: 0.4305\n",
      "valid Loss: 3.2642 Acc: 0.2644\n",
      "\n",
      "Epoch 6/14\n",
      "----------\n",
      "train Loss: 1.8525 Acc: 0.5748\n",
      "valid Loss: 2.8950 Acc: 0.3391\n",
      "\n",
      "Epoch 7/14\n",
      "----------\n",
      "train Loss: 1.1380 Acc: 0.7663\n",
      "valid Loss: 2.3603 Acc: 0.4439\n",
      "\n",
      "Epoch 8/14\n",
      "----------\n",
      "train Loss: 0.9042 Acc: 0.8282\n",
      "valid Loss: 2.2309 Acc: 0.4693\n",
      "\n",
      "Epoch 9/14\n",
      "----------\n",
      "train Loss: 0.8113 Acc: 0.8502\n",
      "valid Loss: 2.1703 Acc: 0.4820\n",
      "\n",
      "Epoch 10/14\n",
      "----------\n",
      "train Loss: 0.7313 Acc: 0.8747\n",
      "valid Loss: 2.1698 Acc: 0.4780\n",
      "\n",
      "Epoch 11/14\n",
      "----------\n",
      "train Loss: 0.6536 Acc: 0.8905\n",
      "valid Loss: 2.1140 Acc: 0.4913\n",
      "\n",
      "Epoch 12/14\n",
      "----------\n",
      "train Loss: 0.6002 Acc: 0.9068\n",
      "valid Loss: 2.0738 Acc: 0.5027\n",
      "\n",
      "Epoch 13/14\n",
      "----------\n",
      "train Loss: 0.5384 Acc: 0.9175\n",
      "valid Loss: 2.0462 Acc: 0.5093\n",
      "\n",
      "Epoch 14/14\n",
      "----------\n",
      "train Loss: 0.4725 Acc: 0.9404\n",
      "valid Loss: 2.0499 Acc: 0.5107\n",
      "\n",
      "Training complete in 151m 44s\n",
      "Best val Acc: 0.510681\n",
      "\n",
      "Saving the model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nSample run: python train.py --mode=finetune\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set training mode\n",
    "#train_mode=args[\"mode\"]\n",
    "\n",
    "# Set the train and validation directory paths\n",
    "train_directory = r'path to train folder'\n",
    "valid_directory = r'path to validation folder'\n",
    "# Set the model save path\n",
    "PATH=\"model_final.pth\" \n",
    "\n",
    "# Batch size\n",
    "bs = 90 \n",
    "# Number of epochs\n",
    "num_epochs = 15\n",
    "# Number of classes\n",
    "num_classes = 300\n",
    "# Number of workers\n",
    "num_cpu = multiprocessing.cpu_count()\n",
    "\n",
    "# Applying transforms to the data\n",
    "image_transforms = { \n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5471, 0.4019, 0.3468],\n",
    "                             [0.1961, 0.1704, 0.1652])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5471, 0.4019, 0.3468],\n",
    "                             [0.1961, 0.1704, 0.1652])\n",
    "    ])\n",
    "}\n",
    " \n",
    "# Load data from folders\n",
    "dataset = {\n",
    "    'train': datasets.ImageFolder(root=train_directory, transform=image_transforms['train']),\n",
    "    'valid': datasets.ImageFolder(root=valid_directory, transform=image_transforms['valid'])\n",
    "}\n",
    " \n",
    "# Size of train and validation data\n",
    "dataset_sizes = {\n",
    "    'train':len(dataset['train']),\n",
    "    'valid':len(dataset['valid'])\n",
    "}\n",
    "\n",
    "# Create iterators for data loading\n",
    "dataloaders = {\n",
    "    'train':data.DataLoader(dataset['train'], batch_size=bs, shuffle=True,\n",
    "                            num_workers=num_cpu, pin_memory=True, drop_last=True),\n",
    "    'valid':data.DataLoader(dataset['valid'], batch_size=bs, shuffle=True,\n",
    "                            num_workers=num_cpu, pin_memory=True, drop_last=True)\n",
    "}\n",
    "\n",
    "# Class names or target labels\n",
    "class_names = dataset['train'].classes\n",
    "print(\"Classes:\", class_names)\n",
    " \n",
    "# Print the train and validation data sizes\n",
    "print(\"Training-set size:\",dataset_sizes['train'],\n",
    "      \"\\nValidation-set size:\", dataset_sizes['valid'])\n",
    "\n",
    "# Set default device as gpu, if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if train_mode=='finetune':\n",
    "    # Load a pretrained model - Resnet18\n",
    "    print(\"\\nLoading resnet18 for finetuning ...\\n\")\n",
    "    model_ft = models.resnet18(pretrained=True)\n",
    "\n",
    "    # Modify fc layers to match num_classes\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "    model_ft.fc = nn.Linear(num_ftrs,num_classes )\n",
    "\n",
    "elif train_mode=='scratch':\n",
    "    # Load a custom model - VGG11\n",
    "    print(\"\\nLoading VGG11 for training from scratch ...\\n\")\n",
    "    model_ft = MyVGG11(in_ch=3,num_classes=11)\n",
    "\n",
    "    # Set number of epochs to a higher value\n",
    "    num_epochs=100\n",
    "\n",
    "elif train_mode=='transfer':\n",
    "    # Load a pretrained model - MobilenetV2\n",
    "    print(\"\\nLoading mobilenetv2 as feature extractor ...\\n\")\n",
    "    model_ft = models.mobilenet_v2(pretrained=True)    \n",
    "\n",
    "    # Freeze all the required layers (i.e except last conv block and fc layers)\n",
    "    for params in list(model_ft.parameters())[0:-5]:\n",
    "        params.requires_grad = False\n",
    "\n",
    "    # Modify fc layers to match num_classes\n",
    "    num_ftrs=model_ft.classifier[-1].in_features\n",
    "    model_ft.classifier=nn.Sequential(\n",
    "        nn.Dropout(p=0.2, inplace=False),\n",
    "        nn.Linear(in_features=num_ftrs, out_features=num_classes, bias=True)\n",
    "        )    \n",
    "\n",
    "# Transfer the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Print model summary\n",
    "print('Model Summary:-\\n')\n",
    "for num, (name, param) in enumerate(model_ft.named_parameters()):\n",
    "    print(num, name, param.requires_grad )\n",
    "summary(model_ft, input_size=(3, 224, 224))\n",
    "print(model_ft)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer \n",
    "# optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "#optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.001)\n",
    "optimizer_ft = optim.AdamW(model_ft.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate decay\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "# Model training routine \n",
    "print(\"\\nTraining:-\\n\")\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=30):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # Tensorboard summary\n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device, non_blocking=True)\n",
    "                labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # Record training loss and accuracy for each phase\n",
    "            if phase == 'train':\n",
    "                writer.add_scalar('Train/Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Train/Accuracy', epoch_acc, epoch)\n",
    "                writer.flush()\n",
    "            else:\n",
    "                writer.add_scalar('Valid/Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Valid/Accuracy', epoch_acc, epoch)\n",
    "                writer.flush()\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=num_epochs)\n",
    "# Save the entire model\n",
    "print(\"\\nSaving the model...\")\n",
    "torch.save(model_ft, PATH)\n",
    "\n",
    "'''\n",
    "Sample run: python train.py --mode=finetune\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43d2ea06-42be-486d-a3f4-def3ec58343c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['0657', '0660', '0661', '0663', '0665', '0666', '0667', '0668', '0669', '0672', '0673', '0674', '0675', '0679', '0680', '0681', '0684', '0687', '0689', '0690', '0692', '0693', '0697', '0699', '0700', '0701', '0703', '0705', '0706', '0707', '0712', '0713', '0714', '0715', '0716', '0722', '0725', '0726', '0728', '0731', '0732', '0734', '0735', '0740', '0741', '0742', '0743', '0747', '0748', '0749', '0750', '0751', '0752', '0753', '0754', '0755', '0757', '0758', '0759', '0760', '0764', '0765', '0766', '0768', '0771', '0772', '0773', '0774', '0775', '0776', '0777', '0778', '0780', '0781', '0782', '0786', '0793', '0796', '0797', '0798', '0802', '0803', '0807', '0808', '0816', '0817', '0818', '0820', '0821', '0822', '0824', '0827', '0828', '0832', '0834', '0841', '0844', '0847', '0848', '0853', '0854', '0858', '0860', '0861', '0864', '0866', '0867', '0868', '0869', '0870', '0873', '0874', '0875', '0878', '0885', '0886', '0887', '0891', '0892', '0894', '0897', '0898', '0899', '0902', '0904', '0905', '0907', '0909', '0910', '0912', '0915', '0920', '0923', '0924', '0925', '0926', '0929', '0930', '0931', '0934', '0935', '0938', '0941', '0943', '0948', '0949', '0950', '0952', '0954', '0955', '0956', '0958', '0962', '0963', '0967', '0968', '0969', '0972', '0976', '0977', '0979', '0984', '0985', '0986', '0987', '0989', '0991', '0993', '0999', '1001', '1005', '1006', '1009', '1011', '1016', '1017', '1019', '1022', '1024', '1025', '1026', '1027', '1028', '1029', '1031', '1033', '1035', '1037', '1038', '1039', '1043', '1047', '1049', '1050', '1057', '1060', '1062', '1063', '1065', '1069', '1070', '1071', '1076', '1079', '1086', '1090', '1091', '1092', '1093', '1096', '1099', '1100', '1101', '1105', '1106', '1107', '1109', '1110', '1112', '1113', '1115', '1123', '1125', '1127', '1128', '1129', '1130', '1137', '1138', '1141', '1142', '1143', '1147', '1148', '1149', '1153', '1154', '1158', '1159', '1161', '1164', '1166', '1171', '1172', '1173', '1174', '1182', '1183', '1184', '1185', '1190', '1191', '1195', '1197', '1198', '1199', '1200', '1201', '1202', '1203', '1214', '1216', '1222', '1228', '1229', '1233', '1235', '1236', '1241', '1243', '1245', '1246', '1247', '1255', '1256', '1258', '1259', '1261', '1262', '1263', '1265', '1267', '1268', '1271', '1272', '1276', '1281', '1282', '1283', '1284', '1286', '1289', '1291', '1292', '1293', '1294', '1297', '1306', '1308', '1309']\n",
      "Training-set size: 8923 \n",
      "Validation-set size: 1498\n",
      "\n",
      "Loading resnet18 for finetuning ...\n",
      "\n",
      "Model Summary:-\n",
      "\n",
      "0 conv1.weight True\n",
      "1 bn1.weight True\n",
      "2 bn1.bias True\n",
      "3 layer1.0.conv1.weight True\n",
      "4 layer1.0.bn1.weight True\n",
      "5 layer1.0.bn1.bias True\n",
      "6 layer1.0.conv2.weight True\n",
      "7 layer1.0.bn2.weight True\n",
      "8 layer1.0.bn2.bias True\n",
      "9 layer1.1.conv1.weight True\n",
      "10 layer1.1.bn1.weight True\n",
      "11 layer1.1.bn1.bias True\n",
      "12 layer1.1.conv2.weight True\n",
      "13 layer1.1.bn2.weight True\n",
      "14 layer1.1.bn2.bias True\n",
      "15 layer2.0.conv1.weight True\n",
      "16 layer2.0.bn1.weight True\n",
      "17 layer2.0.bn1.bias True\n",
      "18 layer2.0.conv2.weight True\n",
      "19 layer2.0.bn2.weight True\n",
      "20 layer2.0.bn2.bias True\n",
      "21 layer2.0.downsample.0.weight True\n",
      "22 layer2.0.downsample.1.weight True\n",
      "23 layer2.0.downsample.1.bias True\n",
      "24 layer2.1.conv1.weight True\n",
      "25 layer2.1.bn1.weight True\n",
      "26 layer2.1.bn1.bias True\n",
      "27 layer2.1.conv2.weight True\n",
      "28 layer2.1.bn2.weight True\n",
      "29 layer2.1.bn2.bias True\n",
      "30 layer3.0.conv1.weight True\n",
      "31 layer3.0.bn1.weight True\n",
      "32 layer3.0.bn1.bias True\n",
      "33 layer3.0.conv2.weight True\n",
      "34 layer3.0.bn2.weight True\n",
      "35 layer3.0.bn2.bias True\n",
      "36 layer3.0.downsample.0.weight True\n",
      "37 layer3.0.downsample.1.weight True\n",
      "38 layer3.0.downsample.1.bias True\n",
      "39 layer3.1.conv1.weight True\n",
      "40 layer3.1.bn1.weight True\n",
      "41 layer3.1.bn1.bias True\n",
      "42 layer3.1.conv2.weight True\n",
      "43 layer3.1.bn2.weight True\n",
      "44 layer3.1.bn2.bias True\n",
      "45 layer4.0.conv1.weight True\n",
      "46 layer4.0.bn1.weight True\n",
      "47 layer4.0.bn1.bias True\n",
      "48 layer4.0.conv2.weight True\n",
      "49 layer4.0.bn2.weight True\n",
      "50 layer4.0.bn2.bias True\n",
      "51 layer4.0.downsample.0.weight True\n",
      "52 layer4.0.downsample.1.weight True\n",
      "53 layer4.0.downsample.1.bias True\n",
      "54 layer4.1.conv1.weight True\n",
      "55 layer4.1.bn1.weight True\n",
      "56 layer4.1.bn1.bias True\n",
      "57 layer4.1.conv2.weight True\n",
      "58 layer4.1.bn2.weight True\n",
      "59 layer4.1.bn2.bias True\n",
      "60 fc.weight True\n",
      "61 fc.bias True\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
      "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
      "             ReLU-14           [-1, 64, 56, 56]               0\n",
      "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
      "             ReLU-17           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
      "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
      "             ReLU-21          [-1, 128, 28, 28]               0\n",
      "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
      "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
      "             ReLU-26          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
      "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
      "             ReLU-30          [-1, 128, 28, 28]               0\n",
      "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
      "             ReLU-33          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
      "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
      "             ReLU-37          [-1, 256, 14, 14]               0\n",
      "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
      "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
      "             ReLU-42          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
      "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
      "             ReLU-46          [-1, 256, 14, 14]               0\n",
      "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
      "             ReLU-49          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
      "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-53            [-1, 512, 7, 7]               0\n",
      "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-58            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
      "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-62            [-1, 512, 7, 7]               0\n",
      "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-65            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 300]         153,900\n",
      "================================================================\n",
      "Total params: 11,330,412\n",
      "Trainable params: 11,330,412\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 62.79\n",
      "Params size (MB): 43.22\n",
      "Estimated Total Size (MB): 106.58\n",
      "----------------------------------------------------------------\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=300, bias=True)\n",
      ")\n",
      "\n",
      "Training:-\n",
      "\n",
      "Epoch 0/29\n",
      "----------\n",
      "train Loss: 5.7691 Acc: 0.0041\n",
      "valid Loss: 5.6248 Acc: 0.0027\n",
      "\n",
      "Epoch 1/29\n",
      "----------\n",
      "train Loss: 5.5403 Acc: 0.0159\n",
      "valid Loss: 5.3897 Acc: 0.0207\n",
      "\n",
      "Epoch 2/29\n",
      "----------\n",
      "train Loss: 5.1302 Acc: 0.0499\n",
      "valid Loss: 5.0629 Acc: 0.0534\n",
      "\n",
      "Epoch 3/29\n",
      "----------\n",
      "train Loss: 4.5806 Acc: 0.1101\n",
      "valid Loss: 4.7156 Acc: 0.0788\n",
      "\n",
      "Epoch 4/29\n",
      "----------\n",
      "train Loss: 3.9566 Acc: 0.1921\n",
      "valid Loss: 4.7612 Acc: 0.0708\n",
      "\n",
      "Epoch 5/29\n",
      "----------\n",
      "train Loss: 3.3422 Acc: 0.2865\n",
      "valid Loss: 4.3719 Acc: 0.1222\n",
      "\n",
      "Epoch 6/29\n",
      "----------\n",
      "train Loss: 2.7679 Acc: 0.3898\n",
      "valid Loss: 3.8141 Acc: 0.2049\n",
      "\n",
      "Epoch 7/29\n",
      "----------\n",
      "train Loss: 2.2894 Acc: 0.4750\n",
      "valid Loss: 3.9601 Acc: 0.2176\n",
      "\n",
      "Epoch 8/29\n",
      "----------\n",
      "train Loss: 1.8735 Acc: 0.5629\n",
      "valid Loss: 3.1191 Acc: 0.3238\n",
      "\n",
      "Epoch 9/29\n",
      "----------\n",
      "train Loss: 1.4986 Acc: 0.6491\n",
      "valid Loss: 3.2129 Acc: 0.3004\n",
      "\n",
      "Epoch 10/29\n",
      "----------\n",
      "train Loss: 1.2184 Acc: 0.7053\n",
      "valid Loss: 3.3673 Acc: 0.3164\n",
      "\n",
      "Epoch 11/29\n",
      "----------\n",
      "train Loss: 1.0051 Acc: 0.7550\n",
      "valid Loss: 2.9274 Acc: 0.3792\n",
      "\n",
      "Epoch 12/29\n",
      "----------\n",
      "train Loss: 0.7953 Acc: 0.8082\n",
      "valid Loss: 2.5910 Acc: 0.4573\n",
      "\n",
      "Epoch 13/29\n",
      "----------\n",
      "train Loss: 0.6518 Acc: 0.8382\n",
      "valid Loss: 2.4387 Acc: 0.4666\n",
      "\n",
      "Epoch 14/29\n",
      "----------\n",
      "train Loss: 0.5420 Acc: 0.8662\n",
      "valid Loss: 2.7178 Acc: 0.4466\n",
      "\n",
      "Epoch 15/29\n",
      "----------\n",
      "train Loss: 0.4582 Acc: 0.8882\n",
      "valid Loss: 2.4334 Acc: 0.4967\n",
      "\n",
      "Epoch 16/29\n",
      "----------\n",
      "train Loss: 0.3624 Acc: 0.9077\n",
      "valid Loss: 2.2997 Acc: 0.5167\n",
      "\n",
      "Epoch 17/29\n",
      "----------\n",
      "train Loss: 0.2937 Acc: 0.9304\n",
      "valid Loss: 2.3328 Acc: 0.5300\n",
      "\n",
      "Epoch 18/29\n",
      "----------\n",
      "train Loss: 0.2352 Acc: 0.9444\n",
      "valid Loss: 2.1807 Acc: 0.5447\n",
      "\n",
      "Epoch 19/29\n",
      "----------\n",
      "train Loss: 0.1762 Acc: 0.9589\n",
      "valid Loss: 2.1920 Acc: 0.5527\n",
      "\n",
      "Epoch 20/29\n",
      "----------\n",
      "train Loss: 0.1543 Acc: 0.9650\n",
      "valid Loss: 2.1610 Acc: 0.5541\n",
      "\n",
      "Epoch 21/29\n",
      "----------\n",
      "train Loss: 0.1305 Acc: 0.9703\n",
      "valid Loss: 2.0293 Acc: 0.5708\n",
      "\n",
      "Epoch 22/29\n",
      "----------\n",
      "train Loss: 0.0971 Acc: 0.9774\n",
      "valid Loss: 2.0648 Acc: 0.5774\n",
      "\n",
      "Epoch 23/29\n",
      "----------\n",
      "train Loss: 0.0800 Acc: 0.9829\n",
      "valid Loss: 1.9992 Acc: 0.5881\n",
      "\n",
      "Epoch 24/29\n",
      "----------\n",
      "train Loss: 0.0701 Acc: 0.9842\n",
      "valid Loss: 2.0137 Acc: 0.5895\n",
      "\n",
      "Epoch 25/29\n",
      "----------\n",
      "train Loss: 0.0622 Acc: 0.9862\n",
      "valid Loss: 1.9472 Acc: 0.5955\n",
      "\n",
      "Epoch 26/29\n",
      "----------\n",
      "train Loss: 0.0624 Acc: 0.9866\n",
      "valid Loss: 1.8896 Acc: 0.6028\n",
      "\n",
      "Epoch 27/29\n",
      "----------\n",
      "train Loss: 0.0577 Acc: 0.9870\n",
      "valid Loss: 1.8944 Acc: 0.5988\n",
      "\n",
      "Epoch 28/29\n",
      "----------\n",
      "train Loss: 0.0543 Acc: 0.9864\n",
      "valid Loss: 1.8704 Acc: 0.6028\n",
      "\n",
      "Epoch 29/29\n",
      "----------\n",
      "train Loss: 0.0516 Acc: 0.9874\n",
      "valid Loss: 1.8598 Acc: 0.6101\n",
      "\n",
      "Training complete in 399m 36s\n",
      "Best val Acc: 0.610147\n",
      "\n",
      "Saving the model...\n",
      "Training completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.utils.data as data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time, os, copy, argparse\n",
    "import multiprocessing\n",
    "from torchsummary import summary\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Set the train and validation directory paths\n",
    "train_directory = r'path to train folder'\n",
    "valid_directory = r'path to validation folder'\n",
    "# Set the model save path\n",
    "PATH = \"model_final.pth\"\n",
    "\n",
    "# Batch size\n",
    "bs = 64\n",
    "# Number of epochs\n",
    "num_epochs = 30\n",
    "# Number of classes\n",
    "num_classes = 300\n",
    "# Number of workers\n",
    "num_cpu = multiprocessing.cpu_count()\n",
    "\n",
    "# Applying transforms to the data\n",
    "image_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomErasing(),  # Ensure RandomErasing is applied after ToTensor()\n",
    "        transforms.Normalize([0.5471, 0.4019, 0.3468], [0.1961, 0.1704, 0.1652])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5471, 0.4019, 0.3468], [0.1961, 0.1704, 0.1652])\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Load data from folders\n",
    "dataset = {\n",
    "    'train': datasets.ImageFolder(root=train_directory, transform=image_transforms['train']),\n",
    "    'valid': datasets.ImageFolder(root=valid_directory, transform=image_transforms['valid'])\n",
    "}\n",
    "\n",
    "# Size of train and validation data\n",
    "dataset_sizes = {\n",
    "    'train': len(dataset['train']),\n",
    "    'valid': len(dataset['valid'])\n",
    "}\n",
    "\n",
    "# Create iterators for data loading\n",
    "dataloaders = {\n",
    "    'train': data.DataLoader(dataset['train'], batch_size=bs, shuffle=True,\n",
    "                             num_workers=num_cpu, pin_memory=True, drop_last=True),\n",
    "    'valid': data.DataLoader(dataset['valid'], batch_size=bs, shuffle=True,\n",
    "                             num_workers=num_cpu, pin_memory=True, drop_last=True)\n",
    "}\n",
    "\n",
    "# Class names or target labels\n",
    "class_names = dataset['train'].classes\n",
    "print(\"Classes:\", class_names)\n",
    "\n",
    "# Print the train and validation data sizes\n",
    "print(\"Training-set size:\", dataset_sizes['train'], \"\\nValidation-set size:\", dataset_sizes['valid'])\n",
    "\n",
    "# Set default device as GPU, if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load a pretrained ResNet18 model\n",
    "print(\"\\nLoading resnet18 for finetuning ...\\n\")\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "\n",
    "# Modify the final fully connected layer to match the number of classes\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "# Transfer the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Print model summary\n",
    "print('Model Summary:-\\n')\n",
    "for num, (name, param) in enumerate(model_ft.named_parameters()):\n",
    "    print(num, name, param.requires_grad)\n",
    "summary(model_ft, input_size=(3, 224, 224))\n",
    "print(model_ft)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer_ft = optim.AdamW(model_ft.parameters(), lr=0.0005, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate scheduler with warm-up\n",
    "exp_lr_scheduler = lr_scheduler.OneCycleLR(optimizer_ft, max_lr=0.001, steps_per_epoch=len(dataloaders['train']), epochs=num_epochs)\n",
    "\n",
    "# Model training routine\n",
    "print(\"\\nTraining:-\\n\")\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=30):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # Tensorboard summary\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device, non_blocking=True)\n",
    "                labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        scheduler.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # Record training loss and accuracy for each phase\n",
    "            if phase == 'train':\n",
    "                writer.add_scalar('Train/Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Train/Accuracy', epoch_acc, epoch)\n",
    "                writer.flush()\n",
    "            else:\n",
    "                writer.add_scalar('Valid/Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Valid/Accuracy', epoch_acc, epoch)\n",
    "                writer.flush()\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=num_epochs)\n",
    "\n",
    "# Save the entire model\n",
    "print(\"\\nSaving the model...\")\n",
    "torch.save(model_ft, PATH)\n",
    "\n",
    "print('Training completed successfully.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3461efac-3f0c-430f-9ffb-6d87bdd22b92",
   "metadata": {},
   "source": [
    "# in this one I just changed the transform function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf851ef8-3763-4458-9b88-4598edc8801a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['0657', '0660', '0661', '0663', '0665', '0666', '0667', '0668', '0669', '0672', '0673', '0674', '0675', '0679', '0680', '0681', '0684', '0687', '0689', '0690', '0692', '0693', '0697', '0699', '0700', '0701', '0703', '0705', '0706', '0707', '0712', '0713', '0714', '0715', '0716', '0722', '0725', '0726', '0728', '0731', '0732', '0734', '0735', '0740', '0741', '0742', '0743', '0747', '0748', '0749', '0750', '0751', '0752', '0753', '0754', '0755', '0757', '0758', '0759', '0760', '0764', '0765', '0766', '0768', '0771', '0772', '0773', '0774', '0775', '0776', '0777', '0778', '0780', '0781', '0782', '0786', '0793', '0796', '0797', '0798', '0802', '0803', '0807', '0808', '0816', '0817', '0818', '0820', '0821', '0822', '0824', '0827', '0828', '0832', '0834', '0841', '0844', '0847', '0848', '0853', '0854', '0858', '0860', '0861', '0864', '0866', '0867', '0868', '0869', '0870', '0873', '0874', '0875', '0878', '0885', '0886', '0887', '0891', '0892', '0894', '0897', '0898', '0899', '0902', '0904', '0905', '0907', '0909', '0910', '0912', '0915', '0920', '0923', '0924', '0925', '0926', '0929', '0930', '0931', '0934', '0935', '0938', '0941', '0943', '0948', '0949', '0950', '0952', '0954', '0955', '0956', '0958', '0962', '0963', '0967', '0968', '0969', '0972', '0976', '0977', '0979', '0984', '0985', '0986', '0987', '0989', '0991', '0993', '0999', '1001', '1005', '1006', '1009', '1011', '1016', '1017', '1019', '1022', '1024', '1025', '1026', '1027', '1028', '1029', '1031', '1033', '1035', '1037', '1038', '1039', '1043', '1047', '1049', '1050', '1057', '1060', '1062', '1063', '1065', '1069', '1070', '1071', '1076', '1079', '1086', '1090', '1091', '1092', '1093', '1096', '1099', '1100', '1101', '1105', '1106', '1107', '1109', '1110', '1112', '1113', '1115', '1123', '1125', '1127', '1128', '1129', '1130', '1137', '1138', '1141', '1142', '1143', '1147', '1148', '1149', '1153', '1154', '1158', '1159', '1161', '1164', '1166', '1171', '1172', '1173', '1174', '1182', '1183', '1184', '1185', '1190', '1191', '1195', '1197', '1198', '1199', '1200', '1201', '1202', '1203', '1214', '1216', '1222', '1228', '1229', '1233', '1235', '1236', '1241', '1243', '1245', '1246', '1247', '1255', '1256', '1258', '1259', '1261', '1262', '1263', '1265', '1267', '1268', '1271', '1272', '1276', '1281', '1282', '1283', '1284', '1286', '1289', '1291', '1292', '1293', '1294', '1297', '1306', '1308', '1309']\n",
      "Training-set size: 8923 \n",
      "Validation-set size: 1498\n",
      "\n",
      "Loading resnet18 for finetuning ...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AMIR HOSSEIN\\.conda\\envs\\py3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\AMIR HOSSEIN\\.conda\\envs\\py3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary:-\n",
      "\n",
      "0 conv1.weight True\n",
      "1 bn1.weight True\n",
      "2 bn1.bias True\n",
      "3 layer1.0.conv1.weight True\n",
      "4 layer1.0.bn1.weight True\n",
      "5 layer1.0.bn1.bias True\n",
      "6 layer1.0.conv2.weight True\n",
      "7 layer1.0.bn2.weight True\n",
      "8 layer1.0.bn2.bias True\n",
      "9 layer1.1.conv1.weight True\n",
      "10 layer1.1.bn1.weight True\n",
      "11 layer1.1.bn1.bias True\n",
      "12 layer1.1.conv2.weight True\n",
      "13 layer1.1.bn2.weight True\n",
      "14 layer1.1.bn2.bias True\n",
      "15 layer2.0.conv1.weight True\n",
      "16 layer2.0.bn1.weight True\n",
      "17 layer2.0.bn1.bias True\n",
      "18 layer2.0.conv2.weight True\n",
      "19 layer2.0.bn2.weight True\n",
      "20 layer2.0.bn2.bias True\n",
      "21 layer2.0.downsample.0.weight True\n",
      "22 layer2.0.downsample.1.weight True\n",
      "23 layer2.0.downsample.1.bias True\n",
      "24 layer2.1.conv1.weight True\n",
      "25 layer2.1.bn1.weight True\n",
      "26 layer2.1.bn1.bias True\n",
      "27 layer2.1.conv2.weight True\n",
      "28 layer2.1.bn2.weight True\n",
      "29 layer2.1.bn2.bias True\n",
      "30 layer3.0.conv1.weight True\n",
      "31 layer3.0.bn1.weight True\n",
      "32 layer3.0.bn1.bias True\n",
      "33 layer3.0.conv2.weight True\n",
      "34 layer3.0.bn2.weight True\n",
      "35 layer3.0.bn2.bias True\n",
      "36 layer3.0.downsample.0.weight True\n",
      "37 layer3.0.downsample.1.weight True\n",
      "38 layer3.0.downsample.1.bias True\n",
      "39 layer3.1.conv1.weight True\n",
      "40 layer3.1.bn1.weight True\n",
      "41 layer3.1.bn1.bias True\n",
      "42 layer3.1.conv2.weight True\n",
      "43 layer3.1.bn2.weight True\n",
      "44 layer3.1.bn2.bias True\n",
      "45 layer4.0.conv1.weight True\n",
      "46 layer4.0.bn1.weight True\n",
      "47 layer4.0.bn1.bias True\n",
      "48 layer4.0.conv2.weight True\n",
      "49 layer4.0.bn2.weight True\n",
      "50 layer4.0.bn2.bias True\n",
      "51 layer4.0.downsample.0.weight True\n",
      "52 layer4.0.downsample.1.weight True\n",
      "53 layer4.0.downsample.1.bias True\n",
      "54 layer4.1.conv1.weight True\n",
      "55 layer4.1.bn1.weight True\n",
      "56 layer4.1.bn1.bias True\n",
      "57 layer4.1.conv2.weight True\n",
      "58 layer4.1.bn2.weight True\n",
      "59 layer4.1.bn2.bias True\n",
      "60 fc.weight True\n",
      "61 fc.bias True\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
      "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
      "             ReLU-14           [-1, 64, 56, 56]               0\n",
      "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
      "             ReLU-17           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
      "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
      "             ReLU-21          [-1, 128, 28, 28]               0\n",
      "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
      "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
      "             ReLU-26          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
      "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
      "             ReLU-30          [-1, 128, 28, 28]               0\n",
      "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
      "             ReLU-33          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
      "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
      "             ReLU-37          [-1, 256, 14, 14]               0\n",
      "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
      "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
      "             ReLU-42          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
      "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
      "             ReLU-46          [-1, 256, 14, 14]               0\n",
      "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
      "             ReLU-49          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
      "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-53            [-1, 512, 7, 7]               0\n",
      "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-58            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
      "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-62            [-1, 512, 7, 7]               0\n",
      "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-65            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 300]         153,900\n",
      "================================================================\n",
      "Total params: 11,330,412\n",
      "Trainable params: 11,330,412\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 62.79\n",
      "Params size (MB): 43.22\n",
      "Estimated Total Size (MB): 106.58\n",
      "----------------------------------------------------------------\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=300, bias=True)\n",
      ")\n",
      "\n",
      "Training:-\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 5.4827 Acc: 0.0202\n",
      "valid Loss: 5.1425 Acc: 0.0294\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 4.2015 Acc: 0.1266\n",
      "valid Loss: 3.8802 Acc: 0.1382\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 2.7990 Acc: 0.3741\n",
      "valid Loss: 2.6903 Acc: 0.3298\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 1.6720 Acc: 0.6254\n",
      "valid Loss: 1.9889 Acc: 0.4987\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 0.9775 Acc: 0.7895\n",
      "valid Loss: 1.5693 Acc: 0.5928\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 0.5270 Acc: 0.8948\n",
      "valid Loss: 1.3727 Acc: 0.6288\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 0.2665 Acc: 0.9563\n",
      "valid Loss: 1.1329 Acc: 0.6929\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 0.0984 Acc: 0.9885\n",
      "valid Loss: 0.8049 Acc: 0.7784\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 0.0545 Acc: 0.9956\n",
      "valid Loss: 0.7732 Acc: 0.7837\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 0.0450 Acc: 0.9964\n",
      "valid Loss: 0.7607 Acc: 0.7891\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 0.0372 Acc: 0.9976\n",
      "valid Loss: 0.7648 Acc: 0.7850\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 0.0295 Acc: 0.9983\n",
      "valid Loss: 0.7348 Acc: 0.7857\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 0.0264 Acc: 0.9985\n",
      "valid Loss: 0.7299 Acc: 0.7977\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.0236 Acc: 0.9984\n",
      "valid Loss: 0.7300 Acc: 0.7944\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.0209 Acc: 0.9985\n",
      "valid Loss: 0.7233 Acc: 0.7944\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.0209 Acc: 0.9985\n",
      "valid Loss: 0.7209 Acc: 0.7944\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.0204 Acc: 0.9984\n",
      "valid Loss: 0.7329 Acc: 0.7997\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.0204 Acc: 0.9984\n",
      "valid Loss: 0.7175 Acc: 0.7991\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.0194 Acc: 0.9985\n",
      "valid Loss: 0.7246 Acc: 0.7957\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.0196 Acc: 0.9985\n",
      "valid Loss: 0.7196 Acc: 0.7984\n",
      "\n",
      "Training complete in 208m 40s\n",
      "Best val Acc: 0.799733\n",
      "\n",
      "Saving the model...\n"
     ]
    }
   ],
   "source": [
    "# Set training mode\n",
    "#train_mode=args[\"mode\"]\n",
    "\n",
    "# Set the train and validation directory paths\n",
    "train_directory = r'path to train folder'\n",
    "valid_directory = r'path to validation folder'\n",
    "# Set the model save path\n",
    "PATH=\"model_final1.pth\" \n",
    "\n",
    "# Batch size\n",
    "bs = 90 \n",
    "# Number of epochs\n",
    "num_epochs = 20\n",
    "# Number of classes\n",
    "num_classes = 300\n",
    "# Number of workers\n",
    "num_cpu = multiprocessing.cpu_count()\n",
    "\n",
    "# Applying transforms to the data\n",
    "image_transforms = { \n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5471, 0.4019, 0.3468],\n",
    "                             [0.1961, 0.1704, 0.1652])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5471, 0.4019, 0.3468],\n",
    "                             [0.1961, 0.1704, 0.1652])\n",
    "    ])\n",
    "}\n",
    " \n",
    "# Load data from folders\n",
    "dataset = {\n",
    "    'train': datasets.ImageFolder(root=train_directory, transform=image_transforms['train']),\n",
    "    'valid': datasets.ImageFolder(root=valid_directory, transform=image_transforms['valid'])\n",
    "}\n",
    " \n",
    "# Size of train and validation data\n",
    "dataset_sizes = {\n",
    "    'train':len(dataset['train']),\n",
    "    'valid':len(dataset['valid'])\n",
    "}\n",
    "\n",
    "# Create iterators for data loading\n",
    "dataloaders = {\n",
    "    'train':data.DataLoader(dataset['train'], batch_size=bs, shuffle=True,\n",
    "                            num_workers=num_cpu, pin_memory=True, drop_last=True),\n",
    "    'valid':data.DataLoader(dataset['valid'], batch_size=bs, shuffle=True,\n",
    "                            num_workers=num_cpu, pin_memory=True, drop_last=True)\n",
    "}\n",
    "\n",
    "# Class names or target labels\n",
    "class_names = dataset['train'].classes\n",
    "print(\"Classes:\", class_names)\n",
    " \n",
    "# Print the train and validation data sizes\n",
    "print(\"Training-set size:\",dataset_sizes['train'],\n",
    "      \"\\nValidation-set size:\", dataset_sizes['valid'])\n",
    "\n",
    "# Set default device as gpu, if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if train_mode=='finetune':\n",
    "    # Load a pretrained model - Resnet18\n",
    "    print(\"\\nLoading resnet18 for finetuning ...\\n\")\n",
    "    model_ft = models.resnet18(pretrained=True)\n",
    "\n",
    "    # Modify fc layers to match num_classes\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "    model_ft.fc = nn.Linear(num_ftrs,num_classes )\n",
    "\n",
    "elif train_mode=='scratch':\n",
    "    # Load a custom model - VGG11\n",
    "    print(\"\\nLoading VGG11 for training from scratch ...\\n\")\n",
    "    model_ft = MyVGG11(in_ch=3,num_classes=11)\n",
    "\n",
    "    # Set number of epochs to a higher value\n",
    "    num_epochs=100\n",
    "\n",
    "elif train_mode=='transfer':\n",
    "    # Load a pretrained model - MobilenetV2\n",
    "    print(\"\\nLoading mobilenetv2 as feature extractor ...\\n\")\n",
    "    model_ft = models.mobilenet_v2(pretrained=True)    \n",
    "\n",
    "    # Freeze all the required layers (i.e except last conv block and fc layers)\n",
    "    for params in list(model_ft.parameters())[0:-5]:\n",
    "        params.requires_grad = False\n",
    "\n",
    "    # Modify fc layers to match num_classes\n",
    "    num_ftrs=model_ft.classifier[-1].in_features\n",
    "    model_ft.classifier=nn.Sequential(\n",
    "        nn.Dropout(p=0.2, inplace=False),\n",
    "        nn.Linear(in_features=num_ftrs, out_features=num_classes, bias=True)\n",
    "        )    \n",
    "\n",
    "# Transfer the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Print model summary\n",
    "print('Model Summary:-\\n')\n",
    "for num, (name, param) in enumerate(model_ft.named_parameters()):\n",
    "    print(num, name, param.requires_grad )\n",
    "summary(model_ft, input_size=(3, 224, 224))\n",
    "print(model_ft)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer \n",
    "# optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "#optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.001)\n",
    "optimizer_ft = optim.AdamW(model_ft.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate decay\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "# Model training routine \n",
    "print(\"\\nTraining:-\\n\")\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=30):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # Tensorboard summary\n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device, non_blocking=True)\n",
    "                labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # Record training loss and accuracy for each phase\n",
    "            if phase == 'train':\n",
    "                writer.add_scalar('Train/Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Train/Accuracy', epoch_acc, epoch)\n",
    "                writer.flush()\n",
    "            else:\n",
    "                writer.add_scalar('Valid/Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Valid/Accuracy', epoch_acc, epoch)\n",
    "                writer.flush()\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=num_epochs)\n",
    "# Save the entire model\n",
    "print(\"\\nSaving the model...\")\n",
    "torch.save(model_ft, PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba53638b-200b-4f57-bc41-5d509888be38",
   "metadata": {},
   "source": [
    "# in the code below in addition to transform func, I used the mean and std deviation of ResNet18 on imagenet dataset and also made the weights of layers to false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88e3b14f-ccba-4a56-8daf-236d26dcf669",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['0657', '0660', '0661', '0663', '0665', '0666', '0667', '0668', '0669', '0672', '0673', '0674', '0675', '0679', '0680', '0681', '0684', '0687', '0689', '0690', '0692', '0693', '0697', '0699', '0700', '0701', '0703', '0705', '0706', '0707', '0712', '0713', '0714', '0715', '0716', '0722', '0725', '0726', '0728', '0731', '0732', '0734', '0735', '0740', '0741', '0742', '0743', '0747', '0748', '0749', '0750', '0751', '0752', '0753', '0754', '0755', '0757', '0758', '0759', '0760', '0764', '0765', '0766', '0768', '0771', '0772', '0773', '0774', '0775', '0776', '0777', '0778', '0780', '0781', '0782', '0786', '0793', '0796', '0797', '0798', '0802', '0803', '0807', '0808', '0816', '0817', '0818', '0820', '0821', '0822', '0824', '0827', '0828', '0832', '0834', '0841', '0844', '0847', '0848', '0853', '0854', '0858', '0860', '0861', '0864', '0866', '0867', '0868', '0869', '0870', '0873', '0874', '0875', '0878', '0885', '0886', '0887', '0891', '0892', '0894', '0897', '0898', '0899', '0902', '0904', '0905', '0907', '0909', '0910', '0912', '0915', '0920', '0923', '0924', '0925', '0926', '0929', '0930', '0931', '0934', '0935', '0938', '0941', '0943', '0948', '0949', '0950', '0952', '0954', '0955', '0956', '0958', '0962', '0963', '0967', '0968', '0969', '0972', '0976', '0977', '0979', '0984', '0985', '0986', '0987', '0989', '0991', '0993', '0999', '1001', '1005', '1006', '1009', '1011', '1016', '1017', '1019', '1022', '1024', '1025', '1026', '1027', '1028', '1029', '1031', '1033', '1035', '1037', '1038', '1039', '1043', '1047', '1049', '1050', '1057', '1060', '1062', '1063', '1065', '1069', '1070', '1071', '1076', '1079', '1086', '1090', '1091', '1092', '1093', '1096', '1099', '1100', '1101', '1105', '1106', '1107', '1109', '1110', '1112', '1113', '1115', '1123', '1125', '1127', '1128', '1129', '1130', '1137', '1138', '1141', '1142', '1143', '1147', '1148', '1149', '1153', '1154', '1158', '1159', '1161', '1164', '1166', '1171', '1172', '1173', '1174', '1182', '1183', '1184', '1185', '1190', '1191', '1195', '1197', '1198', '1199', '1200', '1201', '1202', '1203', '1214', '1216', '1222', '1228', '1229', '1233', '1235', '1236', '1241', '1243', '1245', '1246', '1247', '1255', '1256', '1258', '1259', '1261', '1262', '1263', '1265', '1267', '1268', '1271', '1272', '1276', '1281', '1282', '1283', '1284', '1286', '1289', '1291', '1292', '1293', '1294', '1297', '1306', '1308', '1309']\n",
      "Training-set size: 8923 \n",
      "Validation-set size: 1498\n",
      "\n",
      "Loading resnet18 for fine-tuning ...\n",
      "\n",
      "Model Summary:-\n",
      "\n",
      "0 conv1.weight False\n",
      "1 bn1.weight False\n",
      "2 bn1.bias False\n",
      "3 layer1.0.conv1.weight False\n",
      "4 layer1.0.bn1.weight False\n",
      "5 layer1.0.bn1.bias False\n",
      "6 layer1.0.conv2.weight False\n",
      "7 layer1.0.bn2.weight False\n",
      "8 layer1.0.bn2.bias False\n",
      "9 layer1.1.conv1.weight False\n",
      "10 layer1.1.bn1.weight False\n",
      "11 layer1.1.bn1.bias False\n",
      "12 layer1.1.conv2.weight False\n",
      "13 layer1.1.bn2.weight False\n",
      "14 layer1.1.bn2.bias False\n",
      "15 layer2.0.conv1.weight False\n",
      "16 layer2.0.bn1.weight False\n",
      "17 layer2.0.bn1.bias False\n",
      "18 layer2.0.conv2.weight False\n",
      "19 layer2.0.bn2.weight False\n",
      "20 layer2.0.bn2.bias False\n",
      "21 layer2.0.downsample.0.weight False\n",
      "22 layer2.0.downsample.1.weight False\n",
      "23 layer2.0.downsample.1.bias False\n",
      "24 layer2.1.conv1.weight False\n",
      "25 layer2.1.bn1.weight False\n",
      "26 layer2.1.bn1.bias False\n",
      "27 layer2.1.conv2.weight False\n",
      "28 layer2.1.bn2.weight False\n",
      "29 layer2.1.bn2.bias False\n",
      "30 layer3.0.conv1.weight False\n",
      "31 layer3.0.bn1.weight False\n",
      "32 layer3.0.bn1.bias False\n",
      "33 layer3.0.conv2.weight False\n",
      "34 layer3.0.bn2.weight False\n",
      "35 layer3.0.bn2.bias False\n",
      "36 layer3.0.downsample.0.weight False\n",
      "37 layer3.0.downsample.1.weight False\n",
      "38 layer3.0.downsample.1.bias False\n",
      "39 layer3.1.conv1.weight False\n",
      "40 layer3.1.bn1.weight False\n",
      "41 layer3.1.bn1.bias False\n",
      "42 layer3.1.conv2.weight False\n",
      "43 layer3.1.bn2.weight False\n",
      "44 layer3.1.bn2.bias False\n",
      "45 layer4.0.conv1.weight False\n",
      "46 layer4.0.bn1.weight False\n",
      "47 layer4.0.bn1.bias False\n",
      "48 layer4.0.conv2.weight False\n",
      "49 layer4.0.bn2.weight False\n",
      "50 layer4.0.bn2.bias False\n",
      "51 layer4.0.downsample.0.weight False\n",
      "52 layer4.0.downsample.1.weight False\n",
      "53 layer4.0.downsample.1.bias False\n",
      "54 layer4.1.conv1.weight False\n",
      "55 layer4.1.bn1.weight False\n",
      "56 layer4.1.bn1.bias False\n",
      "57 layer4.1.conv2.weight False\n",
      "58 layer4.1.bn2.weight False\n",
      "59 layer4.1.bn2.bias False\n",
      "60 fc.weight True\n",
      "61 fc.bias True\n",
      "\n",
      "Training:-\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 5.7962 Acc: 0.0087\n",
      "valid Loss: 5.3064 Acc: 0.0134\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 5.1241 Acc: 0.0594\n",
      "valid Loss: 5.0324 Acc: 0.0521\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 4.6688 Acc: 0.1310\n",
      "valid Loss: 4.8673 Acc: 0.0681\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 4.2975 Acc: 0.1954\n",
      "valid Loss: 4.7128 Acc: 0.0848\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 3.9859 Acc: 0.2435\n",
      "valid Loss: 4.5913 Acc: 0.0955\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 3.7379 Acc: 0.2884\n",
      "valid Loss: 4.4929 Acc: 0.1055\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 3.5322 Acc: 0.3198\n",
      "valid Loss: 4.4501 Acc: 0.1128\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 3.2273 Acc: 0.4237\n",
      "valid Loss: 4.3253 Acc: 0.1328\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 3.1665 Acc: 0.4562\n",
      "valid Loss: 4.3300 Acc: 0.1409\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 3.1403 Acc: 0.4583\n",
      "valid Loss: 4.2912 Acc: 0.1475\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 3.1256 Acc: 0.4581\n",
      "valid Loss: 4.2952 Acc: 0.1502\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 3.1030 Acc: 0.4646\n",
      "valid Loss: 4.2902 Acc: 0.1469\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 3.0909 Acc: 0.4622\n",
      "valid Loss: 4.2826 Acc: 0.1442\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 3.0775 Acc: 0.4683\n",
      "valid Loss: 4.2641 Acc: 0.1522\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 3.0304 Acc: 0.4822\n",
      "valid Loss: 4.2938 Acc: 0.1469\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 3.0331 Acc: 0.4803\n",
      "valid Loss: 4.2853 Acc: 0.1442\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 3.0399 Acc: 0.4815\n",
      "valid Loss: 4.2801 Acc: 0.1429\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 3.0269 Acc: 0.4856\n",
      "valid Loss: 4.2895 Acc: 0.1435\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 3.0276 Acc: 0.4855\n",
      "valid Loss: 4.2718 Acc: 0.1395\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 3.0256 Acc: 0.4820\n",
      "valid Loss: 4.2790 Acc: 0.1469\n",
      "\n",
      "Training complete in 108m 6s\n",
      "Best val Acc: 0.152203\n",
      "\n",
      "Saving the model...\n"
     ]
    }
   ],
   "source": [
    "# Set the train and validation directory paths\n",
    "train_directory = r'path to train folder'\n",
    "valid_directory = r'path to validation folder'\n",
    "# Set the model save path\n",
    "PATH = \"model_final_fc.pth\"\n",
    "\n",
    "# Batch size\n",
    "bs = 90\n",
    "# Number of epochs\n",
    "num_epochs = 20\n",
    "# Number of classes\n",
    "num_classes = 300\n",
    "# Number of workers\n",
    "num_cpu = multiprocessing.cpu_count()\n",
    "\n",
    "# Applying transforms to the data\n",
    "image_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Load data from folders\n",
    "dataset = {\n",
    "    'train': datasets.ImageFolder(root=train_directory, transform=image_transforms['train']),\n",
    "    'valid': datasets.ImageFolder(root=valid_directory, transform=image_transforms['valid'])\n",
    "}\n",
    "\n",
    "# Size of train and validation data\n",
    "dataset_sizes = {\n",
    "    'train': len(dataset['train']),\n",
    "    'valid': len(dataset['valid'])\n",
    "}\n",
    "\n",
    "# Create iterators for data loading\n",
    "dataloaders = {\n",
    "    'train': torch.utils.data.DataLoader(dataset['train'], batch_size=bs, shuffle=True,\n",
    "                                         num_workers=num_cpu, pin_memory=True, drop_last=True),\n",
    "    'valid': torch.utils.data.DataLoader(dataset['valid'], batch_size=bs, shuffle=True,\n",
    "                                         num_workers=num_cpu, pin_memory=True, drop_last=True)\n",
    "}\n",
    "\n",
    "# Class names or target labels\n",
    "class_names = dataset['train'].classes\n",
    "print(\"Classes:\", class_names)\n",
    "\n",
    "# Print the train and validation data sizes\n",
    "print(\"Training-set size:\", dataset_sizes['train'],\n",
    "      \"\\nValidation-set size:\", dataset_sizes['valid'])\n",
    "\n",
    "# Set default device as GPU, if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load a pretrained model - Resnet18\n",
    "print(\"\\nLoading resnet18 for fine-tuning ...\\n\")\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze all layers\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the fully connected layer to match num_classes\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "# Only the parameters of the final layer are being optimized\n",
    "optimizer_ft = optim.Adam(model_ft.fc.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "# Transfer the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Print model summary\n",
    "print('Model Summary:-\\n')\n",
    "for num, (name, param) in enumerate(model_ft.named_parameters()):\n",
    "    print(num, name, param.requires_grad)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Learning rate decay\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "# Model training routine\n",
    "print(\"\\nTraining:-\\n\")\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=30):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # Tensorboard summary\n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device, non_blocking=True)\n",
    "                labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # Record training loss and accuracy for each phase\n",
    "            if phase == 'train':\n",
    "                writer.add_scalar('Train/Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Train/Accuracy', epoch_acc, epoch)\n",
    "                writer.flush()\n",
    "            else:\n",
    "                writer.add_scalar('Valid/Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Valid/Accuracy', epoch_acc, epoch)\n",
    "                writer.flush()\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=num_epochs)\n",
    "# Save the entire model\n",
    "print(\"\\nSaving the model...\")\n",
    "torch.save(model_ft, PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8a57d7-76a9-4cb7-a8c7-0396ef592f27",
   "metadata": {},
   "source": [
    "# this is just transform changes and mean and std devtiation of ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f18d6f27-d3f2-4376-92c6-1d5edddf01cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['0657', '0660', '0661', '0663', '0665', '0666', '0667', '0668', '0669', '0672', '0673', '0674', '0675', '0679', '0680', '0681', '0684', '0687', '0689', '0690', '0692', '0693', '0697', '0699', '0700', '0701', '0703', '0705', '0706', '0707', '0712', '0713', '0714', '0715', '0716', '0722', '0725', '0726', '0728', '0731', '0732', '0734', '0735', '0740', '0741', '0742', '0743', '0747', '0748', '0749', '0750', '0751', '0752', '0753', '0754', '0755', '0757', '0758', '0759', '0760', '0764', '0765', '0766', '0768', '0771', '0772', '0773', '0774', '0775', '0776', '0777', '0778', '0780', '0781', '0782', '0786', '0793', '0796', '0797', '0798', '0802', '0803', '0807', '0808', '0816', '0817', '0818', '0820', '0821', '0822', '0824', '0827', '0828', '0832', '0834', '0841', '0844', '0847', '0848', '0853', '0854', '0858', '0860', '0861', '0864', '0866', '0867', '0868', '0869', '0870', '0873', '0874', '0875', '0878', '0885', '0886', '0887', '0891', '0892', '0894', '0897', '0898', '0899', '0902', '0904', '0905', '0907', '0909', '0910', '0912', '0915', '0920', '0923', '0924', '0925', '0926', '0929', '0930', '0931', '0934', '0935', '0938', '0941', '0943', '0948', '0949', '0950', '0952', '0954', '0955', '0956', '0958', '0962', '0963', '0967', '0968', '0969', '0972', '0976', '0977', '0979', '0984', '0985', '0986', '0987', '0989', '0991', '0993', '0999', '1001', '1005', '1006', '1009', '1011', '1016', '1017', '1019', '1022', '1024', '1025', '1026', '1027', '1028', '1029', '1031', '1033', '1035', '1037', '1038', '1039', '1043', '1047', '1049', '1050', '1057', '1060', '1062', '1063', '1065', '1069', '1070', '1071', '1076', '1079', '1086', '1090', '1091', '1092', '1093', '1096', '1099', '1100', '1101', '1105', '1106', '1107', '1109', '1110', '1112', '1113', '1115', '1123', '1125', '1127', '1128', '1129', '1130', '1137', '1138', '1141', '1142', '1143', '1147', '1148', '1149', '1153', '1154', '1158', '1159', '1161', '1164', '1166', '1171', '1172', '1173', '1174', '1182', '1183', '1184', '1185', '1190', '1191', '1195', '1197', '1198', '1199', '1200', '1201', '1202', '1203', '1214', '1216', '1222', '1228', '1229', '1233', '1235', '1236', '1241', '1243', '1245', '1246', '1247', '1255', '1256', '1258', '1259', '1261', '1262', '1263', '1265', '1267', '1268', '1271', '1272', '1276', '1281', '1282', '1283', '1284', '1286', '1289', '1291', '1292', '1293', '1294', '1297', '1306', '1308', '1309']\n",
      "Training-set size: 8923 \n",
      "Validation-set size: 1498\n",
      "\n",
      "Loading resnet18 for finetuning ...\n",
      "\n",
      "Model Summary:-\n",
      "\n",
      "0 conv1.weight True\n",
      "1 bn1.weight True\n",
      "2 bn1.bias True\n",
      "3 layer1.0.conv1.weight True\n",
      "4 layer1.0.bn1.weight True\n",
      "5 layer1.0.bn1.bias True\n",
      "6 layer1.0.conv2.weight True\n",
      "7 layer1.0.bn2.weight True\n",
      "8 layer1.0.bn2.bias True\n",
      "9 layer1.1.conv1.weight True\n",
      "10 layer1.1.bn1.weight True\n",
      "11 layer1.1.bn1.bias True\n",
      "12 layer1.1.conv2.weight True\n",
      "13 layer1.1.bn2.weight True\n",
      "14 layer1.1.bn2.bias True\n",
      "15 layer2.0.conv1.weight True\n",
      "16 layer2.0.bn1.weight True\n",
      "17 layer2.0.bn1.bias True\n",
      "18 layer2.0.conv2.weight True\n",
      "19 layer2.0.bn2.weight True\n",
      "20 layer2.0.bn2.bias True\n",
      "21 layer2.0.downsample.0.weight True\n",
      "22 layer2.0.downsample.1.weight True\n",
      "23 layer2.0.downsample.1.bias True\n",
      "24 layer2.1.conv1.weight True\n",
      "25 layer2.1.bn1.weight True\n",
      "26 layer2.1.bn1.bias True\n",
      "27 layer2.1.conv2.weight True\n",
      "28 layer2.1.bn2.weight True\n",
      "29 layer2.1.bn2.bias True\n",
      "30 layer3.0.conv1.weight True\n",
      "31 layer3.0.bn1.weight True\n",
      "32 layer3.0.bn1.bias True\n",
      "33 layer3.0.conv2.weight True\n",
      "34 layer3.0.bn2.weight True\n",
      "35 layer3.0.bn2.bias True\n",
      "36 layer3.0.downsample.0.weight True\n",
      "37 layer3.0.downsample.1.weight True\n",
      "38 layer3.0.downsample.1.bias True\n",
      "39 layer3.1.conv1.weight True\n",
      "40 layer3.1.bn1.weight True\n",
      "41 layer3.1.bn1.bias True\n",
      "42 layer3.1.conv2.weight True\n",
      "43 layer3.1.bn2.weight True\n",
      "44 layer3.1.bn2.bias True\n",
      "45 layer4.0.conv1.weight True\n",
      "46 layer4.0.bn1.weight True\n",
      "47 layer4.0.bn1.bias True\n",
      "48 layer4.0.conv2.weight True\n",
      "49 layer4.0.bn2.weight True\n",
      "50 layer4.0.bn2.bias True\n",
      "51 layer4.0.downsample.0.weight True\n",
      "52 layer4.0.downsample.1.weight True\n",
      "53 layer4.0.downsample.1.bias True\n",
      "54 layer4.1.conv1.weight True\n",
      "55 layer4.1.bn1.weight True\n",
      "56 layer4.1.bn1.bias True\n",
      "57 layer4.1.conv2.weight True\n",
      "58 layer4.1.bn2.weight True\n",
      "59 layer4.1.bn2.bias True\n",
      "60 fc.weight True\n",
      "61 fc.bias True\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
      "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
      "             ReLU-14           [-1, 64, 56, 56]               0\n",
      "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
      "             ReLU-17           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
      "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
      "             ReLU-21          [-1, 128, 28, 28]               0\n",
      "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
      "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
      "             ReLU-26          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
      "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
      "             ReLU-30          [-1, 128, 28, 28]               0\n",
      "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
      "             ReLU-33          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
      "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
      "             ReLU-37          [-1, 256, 14, 14]               0\n",
      "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
      "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
      "             ReLU-42          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
      "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
      "             ReLU-46          [-1, 256, 14, 14]               0\n",
      "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
      "             ReLU-49          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
      "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-53            [-1, 512, 7, 7]               0\n",
      "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-58            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
      "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-62            [-1, 512, 7, 7]               0\n",
      "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-65            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 300]         153,900\n",
      "================================================================\n",
      "Total params: 11,330,412\n",
      "Trainable params: 11,330,412\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 62.79\n",
      "Params size (MB): 43.22\n",
      "Estimated Total Size (MB): 106.58\n",
      "----------------------------------------------------------------\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=300, bias=True)\n",
      ")\n",
      "\n",
      "Training:-\n",
      "\n",
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 5.5819 Acc: 0.0131\n",
      "valid Loss: 4.9976 Acc: 0.0234\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 4.6744 Acc: 0.0748\n",
      "valid Loss: 4.3142 Acc: 0.0834\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 3.4617 Acc: 0.2431\n",
      "valid Loss: 2.9862 Acc: 0.2744\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 2.1889 Acc: 0.5096\n",
      "valid Loss: 2.4199 Acc: 0.3965\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 1.2896 Acc: 0.7204\n",
      "valid Loss: 1.6737 Acc: 0.5688\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 0.7329 Acc: 0.8476\n",
      "valid Loss: 1.4067 Acc: 0.6368\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 0.3610 Acc: 0.9360\n",
      "valid Loss: 1.1170 Acc: 0.6996\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 0.1296 Acc: 0.9843\n",
      "valid Loss: 0.8336 Acc: 0.7777\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 0.0776 Acc: 0.9934\n",
      "valid Loss: 0.7826 Acc: 0.7891\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 0.0598 Acc: 0.9965\n",
      "valid Loss: 0.7745 Acc: 0.8017\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 0.0503 Acc: 0.9972\n",
      "valid Loss: 0.7546 Acc: 0.8011\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 0.0426 Acc: 0.9975\n",
      "valid Loss: 0.7448 Acc: 0.8011\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 0.0375 Acc: 0.9983\n",
      "valid Loss: 0.7300 Acc: 0.8057\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.0320 Acc: 0.9982\n",
      "valid Loss: 0.7171 Acc: 0.8111\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.0278 Acc: 0.9984\n",
      "valid Loss: 0.7314 Acc: 0.8037\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.0280 Acc: 0.9985\n",
      "valid Loss: 0.7161 Acc: 0.8011\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.0277 Acc: 0.9984\n",
      "valid Loss: 0.7318 Acc: 0.8064\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.0275 Acc: 0.9984\n",
      "valid Loss: 0.7384 Acc: 0.8044\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.0263 Acc: 0.9985\n",
      "valid Loss: 0.7347 Acc: 0.8031\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.0263 Acc: 0.9985\n",
      "valid Loss: 0.7206 Acc: 0.8104\n",
      "\n",
      "Training complete in 202m 13s\n",
      "Best val Acc: 0.811081\n",
      "\n",
      "Saving the model...\n"
     ]
    }
   ],
   "source": [
    "# Set training mode\n",
    "#train_mode=args[\"mode\"]\n",
    "\n",
    "# Set the train and validation directory paths\n",
    "train_directory = r'path to train folder'\n",
    "valid_directory = r'path to validation folder'\n",
    "# Set the model save path\n",
    "PATH=\"model_final1.pth\" \n",
    "\n",
    "# Batch size\n",
    "bs = 90 \n",
    "# Number of epochs\n",
    "num_epochs = 20\n",
    "# Number of classes\n",
    "num_classes = 300\n",
    "# Number of workers\n",
    "num_cpu = multiprocessing.cpu_count()\n",
    "\n",
    "# Applying transforms to the data\n",
    "image_transforms = { \n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    " \n",
    "# Load data from folders\n",
    "dataset = {\n",
    "    'train': datasets.ImageFolder(root=train_directory, transform=image_transforms['train']),\n",
    "    'valid': datasets.ImageFolder(root=valid_directory, transform=image_transforms['valid'])\n",
    "}\n",
    " \n",
    "# Size of train and validation data\n",
    "dataset_sizes = {\n",
    "    'train':len(dataset['train']),\n",
    "    'valid':len(dataset['valid'])\n",
    "}\n",
    "\n",
    "# Create iterators for data loading\n",
    "dataloaders = {\n",
    "    'train':data.DataLoader(dataset['train'], batch_size=bs, shuffle=True,\n",
    "                            num_workers=num_cpu, pin_memory=True, drop_last=True),\n",
    "    'valid':data.DataLoader(dataset['valid'], batch_size=bs, shuffle=True,\n",
    "                            num_workers=num_cpu, pin_memory=True, drop_last=True)\n",
    "}\n",
    "\n",
    "# Class names or target labels\n",
    "class_names = dataset['train'].classes\n",
    "print(\"Classes:\", class_names)\n",
    " \n",
    "# Print the train and validation data sizes\n",
    "print(\"Training-set size:\",dataset_sizes['train'],\n",
    "      \"\\nValidation-set size:\", dataset_sizes['valid'])\n",
    "\n",
    "# Set default device as gpu, if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if train_mode=='finetune':\n",
    "    # Load a pretrained model - Resnet18\n",
    "    print(\"\\nLoading resnet18 for finetuning ...\\n\")\n",
    "    model_ft = models.resnet18(pretrained=True)\n",
    "\n",
    "    # Modify fc layers to match num_classes\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "    model_ft.fc = nn.Linear(num_ftrs,num_classes )\n",
    "\n",
    "elif train_mode=='scratch':\n",
    "    # Load a custom model - VGG11\n",
    "    print(\"\\nLoading VGG11 for training from scratch ...\\n\")\n",
    "    model_ft = MyVGG11(in_ch=3,num_classes=11)\n",
    "\n",
    "    # Set number of epochs to a higher value\n",
    "    num_epochs=100\n",
    "\n",
    "elif train_mode=='transfer':\n",
    "    # Load a pretrained model - MobilenetV2\n",
    "    print(\"\\nLoading mobilenetv2 as feature extractor ...\\n\")\n",
    "    model_ft = models.mobilenet_v2(pretrained=True)    \n",
    "\n",
    "    # Freeze all the required layers (i.e except last conv block and fc layers)\n",
    "    for params in list(model_ft.parameters())[0:-5]:\n",
    "        params.requires_grad = False\n",
    "\n",
    "    # Modify fc layers to match num_classes\n",
    "    num_ftrs=model_ft.classifier[-1].in_features\n",
    "    model_ft.classifier=nn.Sequential(\n",
    "        nn.Dropout(p=0.2, inplace=False),\n",
    "        nn.Linear(in_features=num_ftrs, out_features=num_classes, bias=True)\n",
    "        )    \n",
    "\n",
    "# Transfer the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Print model summary\n",
    "print('Model Summary:-\\n')\n",
    "for num, (name, param) in enumerate(model_ft.named_parameters()):\n",
    "    print(num, name, param.requires_grad )\n",
    "summary(model_ft, input_size=(3, 224, 224))\n",
    "print(model_ft)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer \n",
    "# optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "#optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.001)\n",
    "optimizer_ft = optim.AdamW(model_ft.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate decay\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "# Model training routine \n",
    "print(\"\\nTraining:-\\n\")\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=30):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # Tensorboard summary\n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device, non_blocking=True)\n",
    "                labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # Record training loss and accuracy for each phase\n",
    "            if phase == 'train':\n",
    "                writer.add_scalar('Train/Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Train/Accuracy', epoch_acc, epoch)\n",
    "                writer.flush()\n",
    "            else:\n",
    "                writer.add_scalar('Valid/Loss', epoch_loss, epoch)\n",
    "                writer.add_scalar('Valid/Accuracy', epoch_acc, epoch)\n",
    "                writer.flush()\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=num_epochs)\n",
    "# Save the entire model\n",
    "print(\"\\nSaving the model...\")\n",
    "torch.save(model_ft, PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db0cb8b-1a72-490c-a540-1b4b9b905423",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
